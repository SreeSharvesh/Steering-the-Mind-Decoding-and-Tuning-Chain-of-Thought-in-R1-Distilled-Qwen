## Steering the Mind: Decoding and Tuning Chain-of-Thought in R1-Distilled Qwen

This project explores **how fine-tuning affects hallucination patterns** in language models. Specifically, we are investigating:
- **Does fine-tuning improve factual accuracy or just suppress confident wrong answers?**
- **How do model internals change before and after fine-tuning?**
- **Can we reverse engineer fine-tuned layers to understand these changes?**

To achieve this, we are fine-tuning the **Qwen 1.5B** model on **mathematical reasoning datasets** and analyzing changes in hallucination patterns.

